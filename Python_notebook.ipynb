{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d59fb1",
   "metadata": {},
   "source": [
    "## JupyterHUB: Python notebook\n",
    "\n",
    "In this notebook we will create an example of a workflow in Python. We will use the classic MNIST deep learning example. For this example, we are using a locally saved version of the dataset, to show that we can read and write data from our own folders on VSC_DATA.\n",
    "\n",
    "### Loading and adding packages\n",
    "\n",
    "First of all, we will start by loading the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1e1fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e57e70",
   "metadata": {},
   "source": [
    "As we want to use a GPU, we should check whether or not we have the requested GPU available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc038c",
   "metadata": {},
   "source": [
    "### Getting data from your VSC_DATA folder\n",
    "\n",
    "If you need to access a dataset on the cluster, you should store it in your VSC_DATA folder, as this is the standard location for the JupyterHUB. The actual process of reading in your data is easy and exactly the same as usual. In this case we are using a pickle dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02981976",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('py_notebook_ex')\n",
    "with open('mnist.pickle', 'rb') as data:\n",
    "    mnist_dataset = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a94c20",
   "metadata": {},
   "source": [
    "Data exploration is quite easy here as well. The notebook allows plotting as easy as in other Python IDEs. We could use for example matplotlib to plot the first training example image. \n",
    "\n",
    "As we didn't install the matplotlib package yet, we should use conda to install it. Jupyter also allows to do this install from within the workbook. Just provide the standard 'conda install' command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-particular",
   "metadata": {},
   "source": [
    "As the warning above mentions, you should restart your kernel for the package to be available. If we now check the installed packages, we see that matplotlib is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-module",
   "metadata": {},
   "source": [
    "All conda commands can be run from here. Some things you need to be aware of:\n",
    " - You can only run one conda command per cell\n",
    " - Don't mix usage of conda in a terminal and here. This can cause strange behaviour and even corrupt your kernel. Or you \n",
    "   install the packages in the terminal before launching the kernel or you install them here and restart the kernel afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-survivor",
   "metadata": {},
   "source": [
    "Now that it is installed, we can import matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-hawaii",
   "metadata": {},
   "source": [
    "### Training and testing the model\n",
    "\n",
    "Here, we will create the appropriate datasets, plot a first example and then create the model and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist_dataset\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56281d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1497583",
   "metadata": {},
   "source": [
    "The model can now be compiled and trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39env",
   "language": "python",
   "name": "p39env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
